{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we'll provide a detailed tutorial how one can process multiple chunks by distributing the workload between all the available Intel XPUs."
      ],
      "metadata": {
        "id": "0iCtisd5N229"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installation\n",
        "\n",
        "https://github.com/intel/intel-extension-for-transformers\n",
        "\n",
        "```pip install intel-extension-for-pytorch```\n",
        "\n",
        "follow this README\n",
        "\n",
        "https://github.com/intel/intel-extension-for-pytorch/tree/xpu-main"
      ],
      "metadata": {
        "id": "fpQxTpmTOEy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing necessary libraries. We'll be using Intel edition of the transformers libray to quantise and load the model"
      ],
      "metadata": {
        "id": "NH5CfwmeOe90"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig5teXC4NYal"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from accelerate import PartialState\n",
        "from intel_extension_for_transformers.transformers.modeling import AutoModelForCausalLM\n",
        "import intel_extension_for_pytorch as ipex"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the model, tokenizer, quantise it and input sentences"
      ],
      "metadata": {
        "id": "5QwrjiWZOo-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"what's the capital of England?\", \"what is the tallest mountain?\", \"who is the president of the USA?\"]\n",
        "\n",
        "model_name = \"BAAI/bge-m3\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map=\"xpu\", trust_remote_code=True, use_llm_runtime=False)"
      ],
      "metadata": {
        "id": "AE6h-zycOyCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distributing the workload"
      ],
      "metadata": {
        "id": "XTH4VvwsPWJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we're doing a cheatcode to hardcoding the sentences on each XPU rather than splitting using accelerate\n",
        "\n",
        "https://huggingface.co/docs/accelerate/en/usage_guides/distributed_inference"
      ],
      "metadata": {
        "id": "c-ATRs9KQSDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distributed_state = PartialState()\n",
        "\n",
        "device = torch.device(f\"xpu:{distributed_state.process_index}\")\n",
        "model.to(device)\n",
        "\n",
        "if distributed_state.process_index == 0:\n",
        "    subset_sentences = [\"what's the capital of England?\"]\n",
        "elif distributed_state.process_index == 1:\n",
        "    subset_sentences = [\"who is the president of the USA?\"]\n",
        "elif distributed_state.process_index == 2:\n",
        "    subset_sentences = [\"what is the tallest mountain?\"]\n",
        "else:\n",
        "    subset_sentences = []"
      ],
      "metadata": {
        "id": "zGJkxP2tPa2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, getting the embeddings back"
      ],
      "metadata": {
        "id": "zqeF7TlBQnGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if subset_sentences:\n",
        "    subset_inputs = tokenizer(subset_sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    subset_inputs = {key: tensor.to(device) for key, tensor in subset_inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**subset_inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    embeddings = logits.mean(dim=1)\n",
        "\n",
        "    print(f\"Process {distributed_state.process_index} embeddings:\")\n",
        "    print(embeddings)\n",
        "else:\n",
        "    print(f\"Process {distributed_state.process_index} has no sentences to process.\")"
      ],
      "metadata": {
        "id": "qOGuuQj4Qh57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to use this file use ```accelerate launch [scriptname]```"
      ],
      "metadata": {
        "id": "88D6SIA1QqVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "output\n",
        "\n",
        "```bash\n",
        "Process 1 embeddings:\n",
        "tensor([[-10.1953,   0.1705,   0.0363,  ...,  -5.0781,   0.6475,  -3.7539]],\n",
        "       device='xpu:1', dtype=torch.float16)\n",
        "Process 0 embeddings:\n",
        "tensor([[-6.3789,  0.2463, -9.2734,  ..., -3.4590, -0.7021, -4.2773]],\n",
        "       device='xpu:0', dtype=torch.float16)\n",
        "Process 2 embeddings:\n",
        "tensor([[-4.9922, -0.2871, -2.2910,  ..., -5.4102,  1.5928, -4.4609]],\n",
        "       device='xpu:2', dtype=torch.float16)\n",
        "```"
      ],
      "metadata": {
        "id": "HkPP1xOZQx4n"
      }
    }
  ]
}